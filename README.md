# Neural Network from Scratch

This project implements a neural network and computational graph from scratch, featuring backpropagation with automatic differentiation and gradient calculation via the chain rule. Inspired by Andrej Karpathy's Micrograd, it validates correctness by comparing results with PyTorch.

- **Custom Neural Network Implementation**
  - Forward and backward passes
  - Automatic differentiation
  - Backpropagation
  - Gradient descent optimization
- **Computational Graph**
  - Tracks gradients efficiently
  - Implements operations similar to PyTorch’s autograd
- **Data Visualization**
  - Graphs forward and backward passes
  - Helps explain complex neural network operations

## Technologies Used

This project was created to learn the respective technologies in a real applicable sense.

- Python
- NumPy (for tensor operations)
- Matplotlib (for visualizing training progress)

## License

[MIT](https://choosealicense.com/licenses/mit/)

## Acknowledgments
Inspired by [Andrej Karpathy’s Micrograd](https://github.com/karpathy/micrograd)
